{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql import SparkSession # to work with dataframes\n",
    "\n",
    "sqlContext = SparkSession.builder.appName(\"test\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "import pyspark.sql.functions as F # to work with dataframes siimilar to rdd.map()\n",
    "\n",
    "import random\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit_ID</th>\n",
       "      <th>Incident_Number</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Received_DtTm</th>\n",
       "      <th>On_Scene_DtTm</th>\n",
       "      <th>Available_DtTm</th>\n",
       "      <th>Zipcode_of_Incident</th>\n",
       "      <th>Station_Area</th>\n",
       "      <th>Box</th>\n",
       "      <th>Original_Priority</th>\n",
       "      <th>Call_Type_Group</th>\n",
       "      <th>Number_of_Alarms</th>\n",
       "      <th>Unit_Type</th>\n",
       "      <th>Fire_Prevention_District</th>\n",
       "      <th>Supervisor_District</th>\n",
       "      <th>Location</th>\n",
       "      <th>RowID</th>\n",
       "      <th>Received_month</th>\n",
       "      <th>Received_Hour</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E02</td>\n",
       "      <td>6040419</td>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>2006-05-26 05:54:00</td>\n",
       "      <td>2006-05-26 05:58:00</td>\n",
       "      <td>2006-05-26 06:09:00</td>\n",
       "      <td>94133</td>\n",
       "      <td>02</td>\n",
       "      <td>1354</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>ENGINE</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>(37.7959000461735, -122.410798369528)</td>\n",
       "      <td>061460044-E02</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B06</td>\n",
       "      <td>9052082</td>\n",
       "      <td>Structure Fire</td>\n",
       "      <td>2009-06-25 01:07:00</td>\n",
       "      <td>2009-06-25 01:11:00</td>\n",
       "      <td>2009-06-25 01:28:00</td>\n",
       "      <td>94110</td>\n",
       "      <td>11</td>\n",
       "      <td>0555</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>CHIEF</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>(37.7479996809997, -122.420821864913)</td>\n",
       "      <td>091760166-B06</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M01</td>\n",
       "      <td>1011894</td>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>2001-02-08 01:40:00</td>\n",
       "      <td>2001-02-08 01:51:00</td>\n",
       "      <td>2001-02-08 02:25:00</td>\n",
       "      <td>94103</td>\n",
       "      <td>27</td>\n",
       "      <td>2316</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>MEDIC</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>(37.7812876870909, -122.411318055371)</td>\n",
       "      <td>010390227-M01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67</td>\n",
       "      <td>11056264</td>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>2011-06-20 03:20:00</td>\n",
       "      <td>2011-06-20 03:42:00</td>\n",
       "      <td>2011-06-20 04:59:00</td>\n",
       "      <td>94102</td>\n",
       "      <td>01</td>\n",
       "      <td>1453</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>MEDIC</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>(37.7841410121878, -122.410951620282)</td>\n",
       "      <td>111710038-67</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E43</td>\n",
       "      <td>2066310</td>\n",
       "      <td>Structure Fire</td>\n",
       "      <td>2002-08-11 06:22:00</td>\n",
       "      <td>2002-08-11 06:27:00</td>\n",
       "      <td>2002-08-11 06:38:00</td>\n",
       "      <td>94112</td>\n",
       "      <td>32</td>\n",
       "      <td>6114</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>ENGINE</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>(37.728489442639, -122.429995375728)</td>\n",
       "      <td>022230050-E43</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unit_ID  Incident_Number         Call_Type       Received_DtTm  \\\n",
       "0     E02          6040419  Medical Incident 2006-05-26 05:54:00   \n",
       "1     B06          9052082    Structure Fire 2009-06-25 01:07:00   \n",
       "2     M01          1011894  Medical Incident 2001-02-08 01:40:00   \n",
       "3      67         11056264  Medical Incident 2011-06-20 03:20:00   \n",
       "4     E43          2066310    Structure Fire 2002-08-11 06:22:00   \n",
       "\n",
       "        On_Scene_DtTm      Available_DtTm  Zipcode_of_Incident Station_Area  \\\n",
       "0 2006-05-26 05:58:00 2006-05-26 06:09:00                94133           02   \n",
       "1 2009-06-25 01:11:00 2009-06-25 01:28:00                94110           11   \n",
       "2 2001-02-08 01:51:00 2001-02-08 02:25:00                94103           27   \n",
       "3 2011-06-20 03:42:00 2011-06-20 04:59:00                94102           01   \n",
       "4 2002-08-11 06:27:00 2002-08-11 06:38:00                94112           32   \n",
       "\n",
       "    Box Original_Priority Call_Type_Group  Number_of_Alarms Unit_Type  \\\n",
       "0  1354                 3            None                 1    ENGINE   \n",
       "1  0555                 3            None                 1     CHIEF   \n",
       "2  2316                 2            None                 1     MEDIC   \n",
       "3  1453                 1            None                 1     MEDIC   \n",
       "4  6114                 3            None                 1    ENGINE   \n",
       "\n",
       "  Fire_Prevention_District Supervisor_District  \\\n",
       "0                        1                   3   \n",
       "1                        6                   9   \n",
       "2                        3                   6   \n",
       "3                        3                   6   \n",
       "4                        9                  11   \n",
       "\n",
       "                                Location          RowID  Received_month  \\\n",
       "0  (37.7959000461735, -122.410798369528)  061460044-E02               5   \n",
       "1  (37.7479996809997, -122.420821864913)  091760166-B06               6   \n",
       "2  (37.7812876870909, -122.411318055371)  010390227-M01               2   \n",
       "3  (37.7841410121878, -122.410951620282)   111710038-67               6   \n",
       "4   (37.728489442639, -122.429995375728)  022230050-E43               8   \n",
       "\n",
       "   Received_Hour  response_time  \n",
       "0              5            4.0  \n",
       "1              1            4.0  \n",
       "2              1           11.0  \n",
       "3              3           22.0  \n",
       "4              6            5.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df1 = sqlContext.read.csv('/home/shashank/Documents/gitWorkspace/SFFD-Spark-Project/Data/FireDepartmentBigCleaned.csv')\n",
    "df1 = sqlContext.read.parquet('/home/shashank/Documents/gitWorkspace/SFFD-Spark-Project/Data/FireDepartmentBigHalfCleaned.parquet')\n",
    "df1 = df1.repartition(8)\n",
    "\n",
    "\n",
    "df1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets go over each column and decide if we want to include them in the model and how we deal with null values in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit_ID\n",
    "\n",
    "#### This will not be included in training the model since this is not known ahead of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incident_Number\n",
    "\n",
    "#### This will not be included in training the model since this is a random number assigned to identify the incident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call_Type\n",
    "\n",
    "#### This will be included in training the model. Since this is a categorical variable, we will use one hot encoding here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|           Call_Type| count|\n",
      "+--------------------+------+\n",
      "|         Marine Fire|   237|\n",
      "|Elevator / Escala...| 11101|\n",
      "|  Aircraft Emergency|   482|\n",
      "|Confined Space / ...|   367|\n",
      "|      Administrative|    74|\n",
      "|              Alarms|388555|\n",
      "|Odor (Strange / U...| 10184|\n",
      "|Lightning Strike ...|     6|\n",
      "|Citizen Assist / ...| 61318|\n",
      "|              HazMat|  2723|\n",
      "|Watercraft in Dis...|   448|\n",
      "|           Explosion|  1668|\n",
      "|           Oil Spill|   403|\n",
      "|        Vehicle Fire| 14304|\n",
      "|  Suspicious Package|   235|\n",
      "|   Train / Rail Fire|    21|\n",
      "|Extrication / Ent...|   481|\n",
      "|               Other| 50084|\n",
      "|        Outside Fire| 41920|\n",
      "|   Traffic Collision|144530|\n",
      "|       Assist Police|   895|\n",
      "|Gas Leak (Natural...| 16284|\n",
      "|        Water Rescue| 12189|\n",
      "|   Electrical Hazard| 12376|\n",
      "|   High Angle Rescue|   816|\n",
      "+--------------------+------+\n",
      "only showing top 25 rows\n",
      "\n",
      "+---------+-----+\n",
      "|Call_Type|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.groupBy('Call_Type').count().show(25), df1.filter(df1['Call_Type'].isNull()).groupBy('Call_Type').count().show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since there are no missing values here, we can just use one hot encoding here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Received_DtTm\n",
    "\n",
    "#### This will not be included in training the model. This is the starting point from which we are predicting the response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Received_DtTm.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are no missing values here. Although this won't be used in training, this column has been extracted into other columns which will be used in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On_Scene_DtTm\n",
    "\n",
    "#### This will not be included in training the model since this is essentially what we are predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.On_Scene_DtTm.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No missing values here. If there were any missing values here, we will have to drop the row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available_DtTm\n",
    "\n",
    "#### This will not be included in training the model since this is tied to a unit and there is no way of knowing this ahead of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipcode_of_Incident\n",
    "\n",
    "#### This will be included in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select('Zipcode_of_Incident').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2417"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Zipcode_of_Incident.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_probable_zips = df1.groupBy('Zipcode_of_Incident')\\\n",
    "                        .count().orderBy('count', ascending=False)\\\n",
    "                        .select('Zipcode_of_Incident')\\\n",
    "                        .collect()[:3]\n",
    "\n",
    "most_probable_zips_list = []\n",
    "for x in most_probable_zips:\n",
    "    most_probable_zips_list.append(x[0])\n",
    "    \n",
    "df1 = df1.fillna( { 'Zipcode_of_Incident': random.choice(most_probable_zips_list)} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zipcode is closely related to 'Box' which can be used to fill the missing values. This information will be known at the time of call so zipcode can be used for training the model.\n",
    "\n",
    "#### Currently filled missing with top 3 zip codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station_Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1728, 54)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Station_Area.isNull()).count(), df1.select('Station_Area').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station_Area_Zipcode_of_Incident</th>\n",
       "      <th>94102</th>\n",
       "      <th>94103</th>\n",
       "      <th>94104</th>\n",
       "      <th>94105</th>\n",
       "      <th>94107</th>\n",
       "      <th>94108</th>\n",
       "      <th>94109</th>\n",
       "      <th>94110</th>\n",
       "      <th>94111</th>\n",
       "      <th>94112</th>\n",
       "      <th>94114</th>\n",
       "      <th>94115</th>\n",
       "      <th>94116</th>\n",
       "      <th>94117</th>\n",
       "      <th>94118</th>\n",
       "      <th>94121</th>\n",
       "      <th>94122</th>\n",
       "      <th>94123</th>\n",
       "      <th>94124</th>\n",
       "      <th>94127</th>\n",
       "      <th>94129</th>\n",
       "      <th>94130</th>\n",
       "      <th>94131</th>\n",
       "      <th>94132</th>\n",
       "      <th>94133</th>\n",
       "      <th>94134</th>\n",
       "      <th>94158</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41301</td>\n",
       "      <td>2695</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>null</td>\n",
       "      <td>215</td>\n",
       "      <td>204</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>38</td>\n",
       "      <td>127</td>\n",
       "      <td>133</td>\n",
       "      <td>29</td>\n",
       "      <td>74</td>\n",
       "      <td>70</td>\n",
       "      <td>94</td>\n",
       "      <td>38</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>83</td>\n",
       "      <td>60</td>\n",
       "      <td>24</td>\n",
       "      <td>97</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>52</td>\n",
       "      <td>71</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33651</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>12430</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4440</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08</td>\n",
       "      <td>34</td>\n",
       "      <td>20283</td>\n",
       "      <td>0</td>\n",
       "      <td>614</td>\n",
       "      <td>68021</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>449</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>53</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>2822</td>\n",
       "      <td>0</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1263</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>505</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>1670</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8288</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>371</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1099</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3619</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>60410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>34191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33854</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>581</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>09</td>\n",
       "      <td>3</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2041</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10626</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>56670</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1538</td>\n",
       "      <td>1086</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Station_Area_Zipcode_of_Incident  94102  94103  94104  94105  94107  94108  \\\n",
       "0                               34      0      0      0      0      0      0   \n",
       "1                             null    215    204      9     26     64     38   \n",
       "2                               12      0      2      0      6      0      0   \n",
       "3                               08     34  20283      0    614  68021      5   \n",
       "4                               51   2822      0    239      0      0   1263   \n",
       "5                               19      0      0      0      0      0      0   \n",
       "6                               23      0      0      0      0      0      0   \n",
       "7                               40      0      0      0      0      0      0   \n",
       "8                               09      3    172      0     22   2041      0   \n",
       "9                               15      0      0      0      0      0      0   \n",
       "\n",
       "   94109  94110  94111  94112  94114  94115  94116  94117  94118  94121  \\\n",
       "0      0      0      0      0      0      0      0      0      0  41301   \n",
       "1    127    133     29     74     70     94     38     55     40     83   \n",
       "2      0      0      0      0    909      0      0  33651     50      0   \n",
       "3      0    449    390      0      0      0      0      2     37     53   \n",
       "4      2      0     31      4      0     35      4      0    505     10   \n",
       "5      0    371      0      1      0      0   1099      0      0      0   \n",
       "6      0      3      0      0      0      0   9824      0      0    111   \n",
       "7      0      0      0      0      0      0  33854      0      0      0   \n",
       "8      0  24104      0      0      3      0      0      0      0      0   \n",
       "9      0    205      0  56670      0      0      0      0      0      0   \n",
       "\n",
       "   94122  94123  94124  94127  94129  94130  94131  94132  94133  94134  94158  \n",
       "0   2695      0      0      0      0      0      0      0      0      0      0  \n",
       "1     60     24     97     23      5     12     36     37     52     71     12  \n",
       "2  12430      0      0      0      0      0   4440      0      4      0      0  \n",
       "3    131      0      0      0      0      7      0      0      0      0  10028  \n",
       "4     28   1670      1      0   8288      5      2      0     30      7      0  \n",
       "5      0      0      0   3619      0      0      2  60410      0      0      0  \n",
       "6  34191      0      0      0      0      0      0    440      0      0      0  \n",
       "7   7303      0      0    288      0      0      0    581      0      0      0  \n",
       "8      0      0  10626      0      0   2216      0      0      0     15      0  \n",
       "9      0      0      0   8840      0      0   1538   1086      0      1      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.crosstab('Station_Area', 'Zipcode_of_Incident').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+\n",
      "|Incident_Number|count(Station_Area)|\n",
      "+---------------+-------------------+\n",
      "|        2070329|                  3|\n",
      "|       10110217|                  2|\n",
      "|        1005853|                 21|\n",
      "|       16000185|                  2|\n",
      "|       11074600|                  1|\n",
      "+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = df1.groupBy('Incident_Number').agg({\"Station_Area\":\"count\"}).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Although there are no missing values, this will not be included in training the model since the choosen station is based on availability of units and every station responds to multiple zip codes. A single incident can also have multiple stations responding so we just need to predict the response time considering the closest station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box\n",
    "\n",
    "#### This will not be included in training the model since this is information used only by SFFD for navigation or ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Box.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original_Priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+\n",
      "|Original_Priority|  count|\n",
      "+-----------------+-------+\n",
      "|                3|2737296|\n",
      "|             null|  17834|\n",
      "|                E| 123588|\n",
      "|                B|  21538|\n",
      "|                C|  11610|\n",
      "|                A|  37252|\n",
      "|                1| 141126|\n",
      "|                I|   1885|\n",
      "|                2| 598487|\n",
      "+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Original_Priority').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This will be not included in training the model since this is a value assigned at the time of call and there is no obvious relation with other existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call_Type_Group\n",
    "\n",
    "#### This will be included in training the model. Since this is a categorical variable, we will use one hot encoding here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|     Call_Type_Group|  count|\n",
      "+--------------------+-------+\n",
      "|               Alarm| 346676|\n",
      "|                null|2072871|\n",
      "|Potentially Life-...| 816433|\n",
      "|Non Life-threatening| 397396|\n",
      "|                Fire|  57240|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Call_Type_Group').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------+------+------+------+-------+--------------+------------+-------+------+\n",
      "|Call_Type_Group_Unit_Type|AIRPORT| CHIEF|ENGINE| MEDIC|PRIVATE|RESCUE CAPTAIN|RESCUE SQUAD|SUPPORT| TRUCK|\n",
      "+-------------------------+-------+------+------+------+-------+--------------+------------+-------+------+\n",
      "|                     null|   7931|135479|859260|731341|  68088|         68857|       24496|   3406|174013|\n",
      "|     Non Life-threatening|      2|  2138|107186|201277|  77096|          4706|         703|   1481|  2807|\n",
      "|                    Alarm|      1| 78542|155500|  7207|   1243|           731|        3944|    378| 99130|\n",
      "|                     Fire|      0| 10164| 28953|  3262|    347|          2039|        2436|   1403|  8636|\n",
      "|     Potentially Life-...|      1|  6468|351400|292739| 109669|         26687|        6228|   5752| 17489|\n",
      "+-------------------------+-------+------+------+------+-------+--------------+------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.crosstab('Call_Type_Group', 'Unit_Type').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surprisingly, very few calls to the Fire Department are made because of some fire and more Medic units being requested from Fire Department relative to Fire Engines. \n",
    "#### It's safe to assume the missing  Call_Type_Group_Unit_Type as 'Potentially Life-threatening' since that is by far the most occuring 'Call_Type_Group' and the missing Call_Type_Group is less than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.fillna( { 'Call_Type_Group':'Potentially Life-threatening'} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number_of_Alarms\n",
    "\n",
    "#### This will be included in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Number_of_Alarms.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|Number_of_Alarms|avg(response_time)|\n",
      "+----------------+------------------+\n",
      "|               1|  8.20739726303008|\n",
      "|               3| 18.80141592920354|\n",
      "|               5|18.808988764044944|\n",
      "|               4|19.360347322720695|\n",
      "|               2|15.972736452372938|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Number_of_Alarms').agg({\"response_time\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are no missing values and clearly this is a very good indicator of our target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit_Type\n",
    "\n",
    "#### This will be included in training the model. Since this is a categorical variable, we will use one hot encoding here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Unit_Type.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|     Unit_Type|  count|\n",
      "+--------------+-------+\n",
      "|       AIRPORT|   7935|\n",
      "|         MEDIC|1235826|\n",
      "|         CHIEF| 232791|\n",
      "|  RESCUE SQUAD|  37807|\n",
      "|RESCUE CAPTAIN| 103020|\n",
      "|         TRUCK| 302075|\n",
      "|        ENGINE|1502299|\n",
      "|       SUPPORT|  12420|\n",
      "|       PRIVATE| 256443|\n",
      "+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Unit_Type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|     Unit_Type|avg(response_time)|\n",
      "+--------------+------------------+\n",
      "|       AIRPORT|13.637177063642092|\n",
      "|         MEDIC|10.210417971462002|\n",
      "|         CHIEF| 7.641700065724191|\n",
      "|  RESCUE SQUAD| 7.256936546142248|\n",
      "|RESCUE CAPTAIN| 10.33191613278975|\n",
      "|         TRUCK| 6.812198957212613|\n",
      "|        ENGINE| 6.302190176522783|\n",
      "|       SUPPORT|12.641384863123994|\n",
      "|       PRIVATE|11.200633279130255|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Unit_Type').agg({\"response_time\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are no missing values. We can see some association with the target so this could be a good feature. We can also observe the 'MEDIC' and 'ENGINE' are the most common units which can be used in place of any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire_Prevention_District and Supervisor_District\n",
    "\n",
    "#### This will be included in training the model. Since this is a categorical variable, we will use one hot encoding here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Fire_Prevention_District.isNull()).count(), df1.filter(df1.Supervisor_District.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+\n",
      "|Fire_Prevention_District| count|\n",
      "+------------------------+------+\n",
      "|                       7|231614|\n",
      "|                       3|544686|\n",
      "|                    None| 25831|\n",
      "|                       8|283886|\n",
      "|                       5|269944|\n",
      "|                       6|292827|\n",
      "|                       9|301469|\n",
      "|                       1|409025|\n",
      "|                      10|300578|\n",
      "|                       4|337604|\n",
      "|                       2|693152|\n",
      "+------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Fire_Prevention_District').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|Supervisor_District| count|\n",
      "+-------------------+------+\n",
      "|                  7|201480|\n",
      "|                 11|165249|\n",
      "|                  3|471274|\n",
      "|               None|  2276|\n",
      "|                  8|249244|\n",
      "|                  5|378285|\n",
      "|                  6|986468|\n",
      "|                  9|325983|\n",
      "|                  1|188851|\n",
      "|                 10|341690|\n",
      "|                  4|143811|\n",
      "|                  2|236005|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Supervisor_District').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|Supervisor_District|avg(response_time)|\n",
      "+-------------------+------------------+\n",
      "|                  7| 9.131447290053604|\n",
      "|                 11| 8.693916453352214|\n",
      "|                  3| 7.870773690040189|\n",
      "|               None|13.242970123022847|\n",
      "|                  8| 8.000337019145897|\n",
      "|                  5|7.5794044173044135|\n",
      "|                  6| 8.282187562090204|\n",
      "|                  9|  7.92877235929481|\n",
      "|                  1| 8.359283244462565|\n",
      "|                 10| 8.829471158067253|\n",
      "|                  4| 8.888673328187691|\n",
      "|                  2| 8.022414779347896|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Supervisor_District').agg({\"response_time\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------+\n",
      "|Fire_Prevention_District|avg(response_time)|\n",
      "+------------------------+------------------+\n",
      "|                       7|  8.43057846244182|\n",
      "|                       3| 8.426726591100193|\n",
      "|                    None|11.915992412217877|\n",
      "|                       8| 9.101998689614845|\n",
      "|                       5| 7.694510713333136|\n",
      "|                       6| 7.938789797388902|\n",
      "|                       9| 8.943931216808362|\n",
      "|                       1| 7.954315750870974|\n",
      "|                      10| 8.732089507548789|\n",
      "|                       4| 7.571542398786744|\n",
      "|                       2| 7.837135289229491|\n",
      "+------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Fire_Prevention_District').agg({\"response_time\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+------+------+------+------+------+------+------+------+------+------+-----+\n",
      "|Supervisor_District_Fire_Prevention_District|     1|    10|     2|     3|     4|     5|     6|     7|     8|     9| None|\n",
      "+--------------------------------------------+------+------+------+------+------+------+------+------+------+------+-----+\n",
      "|                                           8|     0|     0| 78721|     0|     0| 50098|114088|     0|     3|  6334|    0|\n",
      "|                                           4|     0|     0|     0|     0|     0|     0|     0|  2923|140888|     0|    0|\n",
      "|                                          11|     0|     0|     0|     0|     0|     0|     0|     0|     0|165171|   78|\n",
      "|                                           9|     0| 40110| 90419|     0|     0|     0|178049|     0|     0| 17405|    0|\n",
      "|                                           5|     0|     0|114212|     0| 56285|182352|     0| 15899|  9537|     0|    0|\n",
      "|                                          10|     0|260468| 20647| 10354|     0|     0|   641|     0|     0| 48874|  706|\n",
      "|                                           6| 18233|     0|389153|507036| 51400|     0|     0|     0|     0|     0|20646|\n",
      "|                                           1|     0|     0|     0|     0|     0| 11836|     0|175705|  1310|     0|    0|\n",
      "|                                           2| 23598|     0|     0|     0|154445| 20060|     0| 37087|     0|     0|  815|\n",
      "|                                        None|     0|     0|     0|     0|     0|     0|     0|     0|     0|     0| 2276|\n",
      "|                                           7|     0|     0|     0|     0|     0|  5598|    49|     0|132148| 63685|    0|\n",
      "|                                           3|367194|     0|     0| 27296| 75474|     0|     0|     0|     0|     0| 1310|\n",
      "+--------------------------------------------+------+------+------+------+------+------+------+------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.crosstab('Supervisor_District', 'Fire_Prevention_District').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is very little differencebetween response times among the different 'Supervisor_District' or 'Fire_Prevention_District' so we could use some educated guess here to fill the nul lvalues. Above cross tabulation shows, most of the missing 'Fire_Prevention_District' falls across Supervisor_District = 6 and when Supervisor_District = 6, probablity of Fire_Prevention_District falls among [2,3,4]. We could replace the nulls in Fire_Prevention_District with on of [2,3,4] and respectively we can in turn use that to replace nulls in Supervisor_District with 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.replace('None',None)\n",
    "\n",
    "missing_Fire_Prevention_District = [2]*7+[3]*8+[4]\n",
    "\n",
    "df1 = df1.fillna( { 'Fire_Prevention_District':random.choice(missing_Fire_Prevention_District),\\\n",
    "                   'Supervisor_District':6} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location\n",
    "\n",
    "#### This will not be included in training the model since this accurate location is recorded after the incident has been resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RowID\n",
    "\n",
    "#### This will not be included in training the model since this is a random number assigned to identify the row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Received_month\n",
    "\n",
    "#### This will be included in training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Received_Hour\n",
    "\n",
    "#### This will be included in training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## response_time\n",
    "\n",
    "#### This will be included in training the model since this the target we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('label', F.col('response_time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avg_response_history\n",
    "\n",
    "#### This will be included in training the model. This is a very good indicator of response time and can be easily calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Zipcode_of_Incident</th>\n",
       "      <th>Station_Area</th>\n",
       "      <th>Received_DtTm</th>\n",
       "      <th>Box</th>\n",
       "      <th>Original_Priority</th>\n",
       "      <th>Call_Type_Group</th>\n",
       "      <th>Number_of_Alarms</th>\n",
       "      <th>Unit_Type</th>\n",
       "      <th>Fire_Prevention_District</th>\n",
       "      <th>Supervisor_District</th>\n",
       "      <th>Received_month</th>\n",
       "      <th>Received_Hour</th>\n",
       "      <th>response_time</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alarms</td>\n",
       "      <td>94114</td>\n",
       "      <td>06</td>\n",
       "      <td>2011-02-10 10:44:00</td>\n",
       "      <td>5232</td>\n",
       "      <td>3</td>\n",
       "      <td>Potentially Life-threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>CHIEF</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>94109</td>\n",
       "      <td>03</td>\n",
       "      <td>2016-05-05 04:15:00</td>\n",
       "      <td>3121</td>\n",
       "      <td>3</td>\n",
       "      <td>Potentially Life-Threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>PRIVATE</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>94116</td>\n",
       "      <td>40</td>\n",
       "      <td>2015-10-12 05:44:00</td>\n",
       "      <td>7444</td>\n",
       "      <td>3</td>\n",
       "      <td>Potentially Life-Threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>MEDIC</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>94131</td>\n",
       "      <td>26</td>\n",
       "      <td>2011-03-25 09:43:00</td>\n",
       "      <td>5573</td>\n",
       "      <td>3</td>\n",
       "      <td>Potentially Life-threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>ENGINE</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>94102</td>\n",
       "      <td>01</td>\n",
       "      <td>2001-04-19 05:55:00</td>\n",
       "      <td>1453</td>\n",
       "      <td>1</td>\n",
       "      <td>Potentially Life-threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>MEDIC</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Call_Type  Zipcode_of_Incident Station_Area       Received_DtTm  \\\n",
       "0            Alarms                94114           06 2011-02-10 10:44:00   \n",
       "1  Medical Incident                94109           03 2016-05-05 04:15:00   \n",
       "2  Medical Incident                94116           40 2015-10-12 05:44:00   \n",
       "3  Medical Incident                94131           26 2011-03-25 09:43:00   \n",
       "4  Medical Incident                94102           01 2001-04-19 05:55:00   \n",
       "\n",
       "    Box Original_Priority               Call_Type_Group  Number_of_Alarms  \\\n",
       "0  5232                 3  Potentially Life-threatening                 1   \n",
       "1  3121                 3  Potentially Life-Threatening                 1   \n",
       "2  7444                 3  Potentially Life-Threatening                 1   \n",
       "3  5573                 3  Potentially Life-threatening                 1   \n",
       "4  1453                 1  Potentially Life-threatening                 1   \n",
       "\n",
       "  Unit_Type Fire_Prevention_District Supervisor_District  Received_month  \\\n",
       "0     CHIEF                        5                   8               2   \n",
       "1   PRIVATE                        4                   6               5   \n",
       "2     MEDIC                        8                   4              10   \n",
       "3    ENGINE                        6                   8               3   \n",
       "4     MEDIC                        3                   6               4   \n",
       "\n",
       "   Received_Hour  response_time  label  \n",
       "0             10            5.0    5.0  \n",
       "1              4            7.0    7.0  \n",
       "2              5            9.0    9.0  \n",
       "3              9            8.0    8.0  \n",
       "4              5           15.0   15.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1.select(['Call_Type', 'Zipcode_of_Incident', 'Station_Area', 'Received_DtTm',\n",
    "                  'Box', 'Original_Priority', 'Call_Type_Group', \n",
    "                  'Number_of_Alarms', 'Unit_Type', 'Fire_Prevention_District', \n",
    "                  'Supervisor_District', 'Received_month', 'Received_Hour', \n",
    "                  'response_time', 'label'])\n",
    "\n",
    "df1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call_Type  =  0\n",
      "Zipcode_of_Incident  =  0\n",
      "Station_Area  =  1728\n",
      "Received_DtTm  =  0\n",
      "Box  =  262\n",
      "Original_Priority  =  17834\n",
      "Call_Type_Group  =  0\n",
      "Number_of_Alarms  =  0\n",
      "Unit_Type  =  0\n",
      "Fire_Prevention_District  =  0\n",
      "Supervisor_District  =  0\n",
      "Received_month  =  0\n",
      "Received_Hour  =  0\n",
      "response_time  =  0\n",
      "label  =  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3690616"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in df1.columns:\n",
    "    print i,\" = \" , df1.filter(df1[i].isNull()).count()\n",
    "    \n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.filter(df1.Station_Area.isNotNull()).filter(df1.Box.isNotNull()).filter(df1.Original_Priority.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call_Type  =  0\n",
      "Zipcode_of_Incident  =  0\n",
      "Station_Area  =  0\n",
      "Received_DtTm  =  0\n",
      "Box  =  0\n",
      "Original_Priority  =  0\n",
      "Call_Type_Group  =  0\n",
      "Number_of_Alarms  =  0\n",
      "Unit_Type  =  0\n",
      "Fire_Prevention_District  =  0\n",
      "Supervisor_District  =  0\n",
      "Received_month  =  0\n",
      "Received_Hour  =  0\n",
      "response_time  =  0\n",
      "label  =  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3670808"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in df1.columns:\n",
    "    print i,\" = \" , df1.filter(df1[i].isNull()).count()\n",
    "    \n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train samples = ', 3616212)\n",
      "('Test samples = ', 54596)\n"
     ]
    }
   ],
   "source": [
    "train_df = df1.filter(df1['Received_DtTm'] < '2019-01-01 00:00:00' ).drop('Received_DtTm')\n",
    "print('Train samples = ', train_df.count() )\n",
    "\n",
    "test_df = df1.filter(df1['Received_DtTm'] >= '2019-01-01 00:00:00' ).drop('Received_DtTm')\n",
    "print('Test samples = ', test_df.count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_vec_in = ['Call_Type','Zipcode_of_Incident','Call_Type_Group',\\\n",
    "                 'Unit_Type','Fire_Prevention_District',\\\n",
    "                 'Supervisor_District','Received_month','Received_Hour']\n",
    "\n",
    "column_vec_out = ['Call_Type_Vec','Zipcode_of_Incident_Vec','Call_Type_Group_Vec',\\\n",
    "                 'Unit_Type_Vec','Fire_Prevention_District_Vec',\\\n",
    "                 'Supervisor_District_Vec','Received_month_Vec','Received_Hour_Vec']\n",
    "\n",
    "indexers = [StringIndexer(inputCol=x, outputCol=x+'_indexed') \\\n",
    "           for x in column_vec_in]\n",
    "\n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol=x+'_indexed', outputCol=y) \\\n",
    "           for x,y in zip(column_vec_in, column_vec_out)]\n",
    "\n",
    "tmp = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp = [i for sublist in tmp for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_now = ['Call_Type_Vec','Zipcode_of_Incident_Vec','Call_Type_Group_Vec',\\\n",
    "            'Unit_Type_Vec','Fire_Prevention_District_Vec',\\\n",
    "            'Supervisor_District_Vec','Received_month_Vec','Received_Hour_Vec',\\\n",
    "            ]\n",
    "\n",
    "assembler_features = VectorAssembler(inputCols=cols_now, outputCol='features')\n",
    "\n",
    "tmp += [assembler_features]\n",
    "pipeline = Pipeline(stages=tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_train_df = pipeline.fit(train_df).transform(train_df).select(['label','features'])\n",
    "transformed_train_df.cache()\n",
    "\n",
    "transformed_test_df = pipeline.fit(test_df).transform(test_df).select(['label','features'])\n",
    "transformed_test_df.cache()\n",
    "transformed_test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3616212, 54596)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train_df.count(), transformed_test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.79654\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",numTrees=1, maxDepth=1, seed=2)\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = rf.fit(transformed_train_df)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(transformed_test_df)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1998.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 448.0 failed 1 times, most recent failure: Lost task 4.0 in stage 448.0 (TID 5056, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 101, y.size = 118\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:707)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:644)\n\tat org.apache.spark.ml.PredictionModel$$anonfun$1.apply(Predictor.scala:215)\n\tat org.apache.spark.ml.PredictionModel$$anonfun$1.apply(Predictor.scala:214)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:57)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:54)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:109)\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:86)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 101, y.size = 118\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:707)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:644)\n\tat org.apache.spark.ml.PredictionModel$$anonfun$1.apply(Predictor.scala:215)\n\tat org.apache.spark.ml.PredictionModel$$anonfun$1.apply(Predictor.scala:214)\n\t... 34 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-3fd60107a944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m evaluator = RegressionEvaluator(\n\u001b[1;32m     15\u001b[0m     labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Root Mean Squared Error (RMSE) on test data = %g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/pyspark/ml/evaluation.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/pyspark/ml/evaluation.pyc\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1998.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 448.0 failed 1 times, most recent failure: Lost task 4.0 in stage 448.0 (TID 5056, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 101, y.size = 118\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:707)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:644)\n\tat org.apache.spark.ml.PredictionModel$$anonfun$1.apply(Predictor.scala:215)\n\tat org.apache.spark.ml.PredictionModel$$anonfun$1.apply(Predictor.scala:214)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:57)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:54)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:109)\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:86)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 101, y.size = 118\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:707)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:644)\n\tat org.apache.spark.ml.PredictionModel$$anonfun$1.apply(Predictor.scala:215)\n\tat org.apache.spark.ml.PredictionModel$$anonfun$1.apply(Predictor.scala:214)\n\t... 34 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = LinearRegression(featuresCol=\"features\")\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = rf.fit(transformed_train_df)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(transformed_test_df)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae and rmse and do logisticR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
