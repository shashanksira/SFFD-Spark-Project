{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql import SparkSession # to work with dataframes\n",
    "\n",
    "sqlContext = SparkSession.builder.appName(\"test\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "import pyspark.sql.functions as F # to work with dataframes siimilar to rdd.map()\n",
    "\n",
    "import random\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit_ID</th>\n",
       "      <th>Incident_Number</th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Received_DtTm</th>\n",
       "      <th>On_Scene_DtTm</th>\n",
       "      <th>Available_DtTm</th>\n",
       "      <th>Zipcode_of_Incident</th>\n",
       "      <th>Station_Area</th>\n",
       "      <th>Box</th>\n",
       "      <th>Original_Priority</th>\n",
       "      <th>Call_Type_Group</th>\n",
       "      <th>Number_of_Alarms</th>\n",
       "      <th>Unit_Type</th>\n",
       "      <th>Fire_Prevention_District</th>\n",
       "      <th>Supervisor_District</th>\n",
       "      <th>Location</th>\n",
       "      <th>RowID</th>\n",
       "      <th>Received_month</th>\n",
       "      <th>Received_Hour</th>\n",
       "      <th>response_time</th>\n",
       "      <th>avg_response_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>18070019</td>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>2018-06-14 06:26:00</td>\n",
       "      <td>2018-06-14 06:39:00</td>\n",
       "      <td>2018-06-14 07:47:00</td>\n",
       "      <td>94122</td>\n",
       "      <td>22</td>\n",
       "      <td>7427</td>\n",
       "      <td>2</td>\n",
       "      <td>Non Life-threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>MEDIC</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>(37.75694617689543, -122.47874036788842)</td>\n",
       "      <td>181653157-57</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>18121635</td>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>2018-10-17 04:34:00</td>\n",
       "      <td>2018-10-17 04:44:00</td>\n",
       "      <td>2018-10-17 05:31:00</td>\n",
       "      <td>94115</td>\n",
       "      <td>10</td>\n",
       "      <td>4154</td>\n",
       "      <td>2</td>\n",
       "      <td>Potentially Life-Threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>MEDIC</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>(37.783357968274046, -122.43860138708125)</td>\n",
       "      <td>182902849-55</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>19026457</td>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>2019-03-03 03:01:00</td>\n",
       "      <td>2019-03-03 03:06:00</td>\n",
       "      <td>2019-03-03 04:10:00</td>\n",
       "      <td>94115</td>\n",
       "      <td>10</td>\n",
       "      <td>4155</td>\n",
       "      <td>3</td>\n",
       "      <td>Potentially Life-Threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>MEDIC</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>(37.78524752592622, -122.43987261265907)</td>\n",
       "      <td>190622092-61</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.740741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E10</td>\n",
       "      <td>18025399</td>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>2018-03-01 12:21:00</td>\n",
       "      <td>2018-03-01 12:28:00</td>\n",
       "      <td>2018-03-01 12:36:00</td>\n",
       "      <td>94115</td>\n",
       "      <td>10</td>\n",
       "      <td>4235</td>\n",
       "      <td>3</td>\n",
       "      <td>Potentially Life-Threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>ENGINE</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>(37.78386547963962, -122.4413702268871)</td>\n",
       "      <td>180600034-E10</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.027027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>18063980</td>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>2018-05-31 03:54:00</td>\n",
       "      <td>2018-05-31 04:02:00</td>\n",
       "      <td>2018-05-31 05:05:00</td>\n",
       "      <td>94107</td>\n",
       "      <td>08</td>\n",
       "      <td>2153</td>\n",
       "      <td>3</td>\n",
       "      <td>Potentially Life-Threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>MEDIC</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>(37.78118776896415, -122.3913514497218)</td>\n",
       "      <td>181512567-52</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.684211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unit_ID  Incident_Number         Call_Type       Received_DtTm  \\\n",
       "0      57         18070019  Medical Incident 2018-06-14 06:26:00   \n",
       "1      55         18121635  Medical Incident 2018-10-17 04:34:00   \n",
       "2      61         19026457  Medical Incident 2019-03-03 03:01:00   \n",
       "3     E10         18025399  Medical Incident 2018-03-01 12:21:00   \n",
       "4      52         18063980  Medical Incident 2018-05-31 03:54:00   \n",
       "\n",
       "        On_Scene_DtTm      Available_DtTm  Zipcode_of_Incident Station_Area  \\\n",
       "0 2018-06-14 06:39:00 2018-06-14 07:47:00                94122           22   \n",
       "1 2018-10-17 04:44:00 2018-10-17 05:31:00                94115           10   \n",
       "2 2019-03-03 03:06:00 2019-03-03 04:10:00                94115           10   \n",
       "3 2018-03-01 12:28:00 2018-03-01 12:36:00                94115           10   \n",
       "4 2018-05-31 04:02:00 2018-05-31 05:05:00                94107           08   \n",
       "\n",
       "    Box Original_Priority               Call_Type_Group  Number_of_Alarms  \\\n",
       "0  7427                 2          Non Life-threatening                 1   \n",
       "1  4154                 2  Potentially Life-Threatening                 1   \n",
       "2  4155                 3  Potentially Life-Threatening                 1   \n",
       "3  4235                 3  Potentially Life-Threatening                 1   \n",
       "4  2153                 3  Potentially Life-Threatening                 1   \n",
       "\n",
       "  Unit_Type Fire_Prevention_District Supervisor_District  \\\n",
       "0     MEDIC                        8                   4   \n",
       "1     MEDIC                        5                   5   \n",
       "2     MEDIC                        5                   5   \n",
       "3    ENGINE                        5                   2   \n",
       "4     MEDIC                        3                   6   \n",
       "\n",
       "                                    Location          RowID  Received_month  \\\n",
       "0   (37.75694617689543, -122.47874036788842)   181653157-57               6   \n",
       "1  (37.783357968274046, -122.43860138708125)   182902849-55              10   \n",
       "2   (37.78524752592622, -122.43987261265907)   190622092-61               3   \n",
       "3    (37.78386547963962, -122.4413702268871)  180600034-E10               3   \n",
       "4    (37.78118776896415, -122.3913514497218)   181512567-52               5   \n",
       "\n",
       "   Received_Hour  response_time  avg_response_history  \n",
       "0              6           13.0             10.571429  \n",
       "1              4           10.0              9.722222  \n",
       "2              3            5.0              7.740741  \n",
       "3             12            7.0             10.027027  \n",
       "4              3            8.0             11.684211  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = sqlContext.read.parquet('/home/shashank/Documents/gitWorkspace/SFFD-Spark-Project/Data/FireSmallCleaned.parquet')\n",
    "df1 = df1.repartition(8)\n",
    "\n",
    "\n",
    "df1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets go over each column and decide if we want to include them in the model and how we deal with null values in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit_ID\n",
    "\n",
    "#### This will not be included in training the model since this is not known ahead of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incident_Number\n",
    "\n",
    "#### This will not be included in training the model since this is a random number assigned to identify the incident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call_Type\n",
    "\n",
    "#### This will be included in training the model. Since this is a categorical variable, we will use one hot encoding here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|           Call_Type| count|\n",
      "+--------------------+------+\n",
      "|Elevator / Escala...|   976|\n",
      "|         Marine Fire|    11|\n",
      "|Confined Space / ...|    44|\n",
      "|      Administrative|     2|\n",
      "|              Alarms| 33583|\n",
      "|Odor (Strange / U...|   255|\n",
      "|Citizen Assist / ...|  4382|\n",
      "|              HazMat|   125|\n",
      "|Watercraft in Dis...|    29|\n",
      "|           Explosion|    52|\n",
      "|        Vehicle Fire|   798|\n",
      "|  Suspicious Package|    24|\n",
      "|   Train / Rail Fire|    14|\n",
      "|Extrication / Ent...|    38|\n",
      "|               Other|  3516|\n",
      "|        Outside Fire|  3701|\n",
      "|   Traffic Collision| 11846|\n",
      "|       Assist Police|    33|\n",
      "|Gas Leak (Natural...|  1966|\n",
      "|        Water Rescue|   934|\n",
      "|   Electrical Hazard|  1344|\n",
      "|   High Angle Rescue|    24|\n",
      "|      Structure Fire| 19348|\n",
      "|Industrial Accidents|    54|\n",
      "|    Medical Incident|214334|\n",
      "+--------------------+------+\n",
      "only showing top 25 rows\n",
      "\n",
      "+---------+-----+\n",
      "|Call_Type|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.groupBy('Call_Type').count().show(25), df1.filter(df1['Call_Type'].isNull()).groupBy('Call_Type').count().show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since there are no missing values here, we can just use one hot encoding here."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"Call_Type\", outputCol=\"Call_Type_Indexed\")\n",
    "model = stringIndexer.fit(df1)\n",
    "indexed = model.transform(df1)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Call_Type_Indexed\", outputCol=\"Call_Type_Vec\")\n",
    "df1 = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Received_DtTm\n",
    "\n",
    "#### This will not be included in training the model. This is the starting point from which we are predicting the response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Received_DtTm.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are no missing values here. Although this won't be used in training, this column has been extracted into other columns which will be used in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On_Scene_DtTm\n",
    "\n",
    "#### This will not be included in training the model since this is essentially what we are predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.On_Scene_DtTm.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No missing values here. If there were any missing values here, we will have to drop the row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available_DtTm\n",
    "\n",
    "#### This will not be included in training the model since this is tied to a unit and there is no way of knowing this ahead of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipcode_of_Incident\n",
    "\n",
    "#### This will be included in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select('Zipcode_of_Incident').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Zipcode_of_Incident.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_probable_zips = df1.groupBy('Zipcode_of_Incident')\\\n",
    "                        .count().orderBy('count', ascending=False)\\\n",
    "                        .select('Zipcode_of_Incident')\\\n",
    "                        .collect()[:3]\n",
    "\n",
    "most_probable_zips_list = []\n",
    "for x in most_probable_zips:\n",
    "    most_probable_zips_list.append(x[0])\n",
    "    \n",
    "df1 = df1.fillna( { 'Zipcode_of_Incident': random.choice(most_probable_zips_list)} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zipcode is closely related to 'Box' which can be used to fill the missing values. This information will be known at the time of call so zipcode can be used for training the model.\n",
    "\n",
    "#### Currently filled missing with top 3 zip codes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"Zipcode_of_Incident\", outputCol=\"Zipcode_of_Incident_Indexed\")\n",
    "model = stringIndexer.fit(df1)\n",
    "indexed = model.transform(df1)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Zipcode_of_Incident_Indexed\", outputCol=\"Zipcode_of_Incident_Vec\")\n",
    "df1 = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "most_probable_zips = df1.groupBy('Zipcode_of_Incident')\\\n",
    "                        .count().orderBy('count', ascending=False)\\\n",
    "                        .select('Zipcode_of_Incident')\\\n",
    "                        .collect()[:3]\n",
    "most_probable_zips_list = []\n",
    "for x in most_probable_zips:\n",
    "    most_probable_zips_list.append(x[0])\n",
    "\n",
    "\n",
    "def get_zip(x):\n",
    "    \n",
    "    missing_zip = df1.filter(df1['Box'] == x)\\\n",
    "                    .filter(df1['Zipcode_of_Incident'].isNotNull())\\\n",
    "                    .crosstab('Zipcode_of_Incident', 'Box')\\\n",
    "                    .select('Zipcode_of_Incident_Box').collect()\n",
    "    if(len(missing_zip) == 0):\n",
    "        return random.choice(most_probable_zips_list)\n",
    "    \n",
    "    return float(missing_zip[0][0])\n",
    "\n",
    "\n",
    "    \n",
    "get_zip_udf = F.udf(get_zip, T.FloatType())\n",
    "    \n",
    "\n",
    "df1 = df1.filter(df1['Zipcode_of_Incident'].isNull())\\\n",
    "            .withColumn('Zipcode_of_Incident', get_zip_udf( df1['Box'] )  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station_Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 45)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Station_Area.isNull()).count(), df1.select('Station_Area').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station_Area_Zipcode_of_Incident</th>\n",
       "      <th>94102</th>\n",
       "      <th>94103</th>\n",
       "      <th>94104</th>\n",
       "      <th>94105</th>\n",
       "      <th>94107</th>\n",
       "      <th>94108</th>\n",
       "      <th>94109</th>\n",
       "      <th>94110</th>\n",
       "      <th>94111</th>\n",
       "      <th>94112</th>\n",
       "      <th>94114</th>\n",
       "      <th>94115</th>\n",
       "      <th>94116</th>\n",
       "      <th>94117</th>\n",
       "      <th>94118</th>\n",
       "      <th>94121</th>\n",
       "      <th>94122</th>\n",
       "      <th>94123</th>\n",
       "      <th>94124</th>\n",
       "      <th>94127</th>\n",
       "      <th>94129</th>\n",
       "      <th>94130</th>\n",
       "      <th>94131</th>\n",
       "      <th>94132</th>\n",
       "      <th>94133</th>\n",
       "      <th>94134</th>\n",
       "      <th>94158</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3072</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2435</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08</td>\n",
       "      <td>0</td>\n",
       "      <td>1099</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>271</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>851</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4819</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>685</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2572</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2682</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>603</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>09</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2052</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4581</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>798</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Station_Area_Zipcode_of_Incident  94102  94103  94104  94105  94107  94108  \\\n",
       "0                               34      0      0      0      0      0      0   \n",
       "1                               12      0      0      0      0      0      0   \n",
       "2                               08      0   1099      0      0   5415      0   \n",
       "3                               51    106      0     45      0      0    170   \n",
       "4                               19      0     22      0      0      0      0   \n",
       "5                               23      0      0      0      0      0      0   \n",
       "6                               40      0      0      0      0      0      0   \n",
       "7                               09      0     46      0      7     55      0   \n",
       "8                               15      0      0      0      0      0      0   \n",
       "9                               11      0      0      0     10      0      0   \n",
       "\n",
       "   94109  94110  94111  94112  94114  94115  94116  94117  94118  94121  \\\n",
       "0      0      0      0      0      0      0      0      0      0   3072   \n",
       "1      0      0      0      0     71      0      0   2435     17      0   \n",
       "2      0    175     78      0      0      0      0      0      0      4   \n",
       "3      0      0     14      0      0      8      0      0     63      2   \n",
       "4      0      3      0      0      0      0     45      0      0      0   \n",
       "5      0      0      0      0      0      0    685      0      0      0   \n",
       "6      0      0      0      0      0      0   2682      0      0      0   \n",
       "7      0   2052      0      0      0      0      0      0      0      0   \n",
       "8      0     22      0   4581      0      0      0      0      0      0   \n",
       "9      0   4916      0      0   1091      0      0      0      0      0   \n",
       "\n",
       "   94122  94123  94124  94127  94129  94130  94131  94132  94133  94134  94158  \n",
       "0    168      0      0      0      0      0      0      0      0      0      0  \n",
       "1    984      0      0      0      0      0    255      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0      0      0    762  \n",
       "3      2    271      1      0    851      0      0      0      1      0      0  \n",
       "4      0      0      0    294      0      0      0   4819      0      0      0  \n",
       "5   2572      0      0      0      0      0      0     38      0      0      0  \n",
       "6    603      0      0     11      0      0      0     52      0      0      0  \n",
       "7      0      0   1083      0      0     92      0      0      0      4      0  \n",
       "8      0      0      0    555      0      0     73    166      0      0      0  \n",
       "9      0      0      0      0      0      0    798      0      0      0      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.crosstab('Station_Area', 'Zipcode_of_Incident').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+\n",
      "|Incident_Number|count(Station_Area)|\n",
      "+---------------+-------------------+\n",
      "|       18031325|                  2|\n",
      "|       18134893|                  2|\n",
      "|       18052322|                 11|\n",
      "|       19005278|                  3|\n",
      "|       18097042|                  2|\n",
      "+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = df1.groupBy('Incident_Number').agg({\"Station_Area\":\"count\"}).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Although there are no missing values, this will not be included in training the model since the choosen station is based on availability of units and every station responds to multiple zip codes. A single incident can also have multiple stations responding so we just need to predict the response time considering the closest station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box\n",
    "\n",
    "#### This will not be included in training the model since this is information used only by SFFD for navigation or ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Box.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original_Priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|Original_Priority| count|\n",
      "+-----------------+------+\n",
      "|                3|176784|\n",
      "|             null|     1|\n",
      "|                E|  7938|\n",
      "|                B| 14012|\n",
      "|                C|  8171|\n",
      "|                A| 22454|\n",
      "|                1|     2|\n",
      "|                I|  1269|\n",
      "|                2| 67931|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Original_Priority').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This will be not included in training the model since this is a value assigned at the time of call and there is no obvious relation with other existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call_Type_Group\n",
    "\n",
    "#### This will be included in training the model. Since this is a categorical variable, we will use one hot encoding here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|     Call_Type_Group| count|\n",
      "+--------------------+------+\n",
      "|               Alarm| 63327|\n",
      "|                null|   826|\n",
      "|Potentially Life-...|146110|\n",
      "|Non Life-threatening| 79803|\n",
      "|                Fire|  8496|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Call_Type_Group').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+------+-----+-------+--------------+------------+-------+-----+\n",
      "|Call_Type_Group_Unit_Type|CHIEF|ENGINE|MEDIC|PRIVATE|RESCUE CAPTAIN|RESCUE SQUAD|SUPPORT|TRUCK|\n",
      "+-------------------------+-----+------+-----+-------+--------------+------------+-------+-----+\n",
      "|                     null|   33|   295|  285|     87|            29|          26|     42|   29|\n",
      "|     Non Life-threatening|  438| 18202|42915|  15376|           991|         120|   1327|  434|\n",
      "|                    Alarm|15096| 27213| 1344|    220|           126|         657|     69|18602|\n",
      "|                     Fire| 1257|  4677|  492|     50|           322|         269|    185| 1244|\n",
      "|     Potentially Life-...| 1332| 55542|55399|  19344|          5027|         946|   5524| 2996|\n",
      "+-------------------------+-----+------+-----+-------+--------------+------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.crosstab('Call_Type_Group', 'Unit_Type').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surprisingly, very few calls to the Fire Department are made because of some fire and more Medic units being requested from Fire Department relative to Fire Engines. \n",
    "#### It's safe to assume the missing  Call_Type_Group_Unit_Type as 'Potentially Life-threatening' since that is by far the most occuring 'Call_Type_Group' and the missing Call_Type_Group is less than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.fillna( { 'Call_Type_Group':'Potentially Life-threatening'} )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"Call_Type_Group\", outputCol=\"Call_Type_Group_Indexed\")\n",
    "model = stringIndexer.fit(df1)\n",
    "indexed = model.transform(df1)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Call_Type_Group_Indexed\", outputCol=\"Call_Type_Group_Vec\")\n",
    "df1 = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number_of_Alarms\n",
    "\n",
    "#### This will be included in training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Number_of_Alarms.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|Number_of_Alarms|avg(response_time)|\n",
      "+----------------+------------------+\n",
      "|               1| 9.270422402919786|\n",
      "|               3|19.896296296296295|\n",
      "|               4|25.470588235294116|\n",
      "|               2|16.633217993079583|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Number_of_Alarms').agg({\"response_time\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are no missing values and clearly this is a very good indicator of our target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit_Type\n",
    "\n",
    "#### This will be included in training the model. Since this is a categorical variable, we will use one hot encoding here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Unit_Type.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|     Unit_Type| count|\n",
      "+--------------+------+\n",
      "|         MEDIC|100435|\n",
      "|         CHIEF| 18156|\n",
      "|  RESCUE SQUAD|  2018|\n",
      "|RESCUE CAPTAIN|  6495|\n",
      "|         TRUCK| 23305|\n",
      "|        ENGINE|105929|\n",
      "|       SUPPORT|  7147|\n",
      "|       PRIVATE| 35077|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Unit_Type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|     Unit_Type|avg(response_time)|\n",
      "+--------------+------------------+\n",
      "|         MEDIC|  11.5386468860457|\n",
      "|         CHIEF| 8.325677461996035|\n",
      "|  RESCUE SQUAD| 7.797819623389494|\n",
      "|RESCUE CAPTAIN|11.269745958429562|\n",
      "|         TRUCK| 7.234842308517486|\n",
      "|        ENGINE| 6.937165459883507|\n",
      "|       SUPPORT| 7.779208059325591|\n",
      "|       PRIVATE|11.799127633492033|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Unit_Type').agg({\"response_time\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are no missing values. We can see some association with the target so this could be a good feature. We can also observe the 'MEDIC' and 'ENGINE' are the most common units which can be used in place of any missing values."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"Unit_Type\", outputCol=\"Unit_Type_Indexed\")\n",
    "model = stringIndexer.fit(df1)\n",
    "indexed = model.transform(df1)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Unit_Type_Indexed\", outputCol=\"Unit_Type_Vec\")\n",
    "df1 = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire_Prevention_District and Supervisor_District\n",
    "\n",
    "#### This will be included in training the model. Since this is a categorical variable, we will use one hot encoding here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1.Fire_Prevention_District.isNull()).count(), df1.filter(df1.Supervisor_District.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+\n",
      "|Fire_Prevention_District|count|\n",
      "+------------------------+-----+\n",
      "|                       7|17298|\n",
      "|                       3|46333|\n",
      "|                    None| 2074|\n",
      "|                       8|21489|\n",
      "|                       5|20351|\n",
      "|                       6|21699|\n",
      "|                       9|23086|\n",
      "|                       1|32744|\n",
      "|                      10|23356|\n",
      "|                       4|27538|\n",
      "|                       2|62594|\n",
      "+------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Fire_Prevention_District').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|Supervisor_District|count|\n",
      "+-------------------+-----+\n",
      "|                  7|15515|\n",
      "|                 11|12894|\n",
      "|                  3|37872|\n",
      "|               None|  102|\n",
      "|                  8|20471|\n",
      "|                  5|28129|\n",
      "|                  6|89250|\n",
      "|                  9|24788|\n",
      "|                  1|13888|\n",
      "|                 10|26309|\n",
      "|                  4|10421|\n",
      "|                  2|18923|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Supervisor_District').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|Supervisor_District|avg(response_time)|\n",
      "+-------------------+------------------+\n",
      "|                  7| 9.988269416693523|\n",
      "|                 11| 9.632852489530014|\n",
      "|                  3| 8.979008238276299|\n",
      "|               None|12.715686274509803|\n",
      "|                  8|  8.66401250549558|\n",
      "|                  5|  8.66461658786306|\n",
      "|                  6| 9.379473389355743|\n",
      "|                  9|  8.96679845086332|\n",
      "|                  1| 9.413810483870968|\n",
      "|                 10| 9.976471929757878|\n",
      "|                  4|  9.99971211975818|\n",
      "|                  2| 9.168366538075357|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Supervisor_District').agg({\"response_time\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------+\n",
      "|Fire_Prevention_District|avg(response_time)|\n",
      "+------------------------+------------------+\n",
      "|                       7| 9.476471268354723|\n",
      "|                       3| 9.738329052726998|\n",
      "|                    None|12.217454194792671|\n",
      "|                       8|10.101121504025315|\n",
      "|                       5| 8.668271829394133|\n",
      "|                       6| 8.759482003778976|\n",
      "|                       9| 9.868794940656676|\n",
      "|                       1| 9.098369166870267|\n",
      "|                      10| 9.927812981674945|\n",
      "|                       4| 8.684835500036314|\n",
      "|                       2| 8.804549956864875|\n",
      "+------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('Fire_Prevention_District').agg({\"response_time\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+\n",
      "|Supervisor_District_Fire_Prevention_District|    1|   10|    2|    3|    4|    5|    6|    7|    8|    9|None|\n",
      "+--------------------------------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+\n",
      "|                                           8|    0|    0| 6490|    0|    0| 4549| 8870|    0|    0|  562|   0|\n",
      "|                                           4|    0|    0|    0|    0|    0|    0|    0|  175|10246|    0|   0|\n",
      "|                                          11|    0|    0|    0|    0|    0|    0|    0|    0|    0|12886|   8|\n",
      "|                                           9|    0| 3262| 7454|    0|    0|    0|12768|    0|    0| 1304|   0|\n",
      "|                                           5|    0|    0| 8927|    0| 4347|13144|    0|  951|  760|    0|   0|\n",
      "|                                          10|    0|20094| 1724|  787|    0|    0|   55|    0|    0| 3615|  34|\n",
      "|                                           6| 1460|    0|37999|43165| 4857|    0|    0|    0|    0|    0|1769|\n",
      "|                                           1|    0|    0|    0|    0|    0|  922|    0|12914|   52|    0|   0|\n",
      "|                                           2| 1759|    0|    0|    0|12479| 1377|    0| 3258|    0|    0|  50|\n",
      "|                                        None|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0| 102|\n",
      "|                                           7|    0|    0|    0|    0|    0|  359|    6|    0|10431| 4719|   0|\n",
      "|                                           3|29525|    0|    0| 2381| 5855|    0|    0|    0|    0|    0| 111|\n",
      "+--------------------------------------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.crosstab('Supervisor_District', 'Fire_Prevention_District').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is very little differencebetween response times among the different 'Supervisor_District' or 'Fire_Prevention_District' so we could use some educated guess here to fill the nul lvalues. Above cross tabulation shows, most of the missing 'Fire_Prevention_District' falls across Supervisor_District = 6 and when Supervisor_District = 6, probablity of Fire_Prevention_District falls among [2,3,4]. We could replace the nulls in Fire_Prevention_District with on of [2,3,4] and respectively we can in turn use that to replace nulls in Supervisor_District with 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.replace('None',None)\n",
    "\n",
    "missing_Fire_Prevention_District = [2]*7+[3]*8+[4]\n",
    "\n",
    "df1 = df1.fillna( { 'Fire_Prevention_District':random.choice(missing_Fire_Prevention_District),\\\n",
    "                   'Supervisor_District':6} )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"Fire_Prevention_District\", outputCol=\"Fire_Prevention_District_Indexed\")\n",
    "model = stringIndexer.fit(df1)\n",
    "indexed = model.transform(df1)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Fire_Prevention_District_Indexed\", outputCol=\"Fire_Prevention_District_Vec\")\n",
    "df1 = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"Supervisor_District\", outputCol=\"Supervisor_District_Indexed\")\n",
    "model = stringIndexer.fit(df1)\n",
    "indexed = model.transform(df1)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Supervisor_District_Indexed\", outputCol=\"Supervisor_District_Vec\")\n",
    "df1 = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location\n",
    "\n",
    "#### This will not be included in training the model since this accurate location is recorded after the incident has been resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RowID\n",
    "\n",
    "#### This will not be included in training the model since this is a random number assigned to identify the row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Received_month\n",
    "\n",
    "#### This will be included in training the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"Received_month\", outputCol=\"Received_month_Indexed\")\n",
    "model = stringIndexer.fit(df1)\n",
    "indexed = model.transform(df1)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Received_month_Indexed\", outputCol=\"Received_month_Vec\")\n",
    "df1 = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Received_Hour\n",
    "\n",
    "#### This will be included in training the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"Received_Hour\", outputCol=\"Received_Hour_Indexed\")\n",
    "model = stringIndexer.fit(df1)\n",
    "indexed = model.transform(df1)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Received_Hour_Indexed\", outputCol=\"Received_Hour_Vec\")\n",
    "df1 = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## response_time\n",
    "\n",
    "#### This will be included in training the model since this the target we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('label', F.col('response_time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avg_response_history\n",
    "\n",
    "#### This will be included in training the model. This is a very good indicator of response time and can be easily calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Call_Type</th>\n",
       "      <th>Zipcode_of_Incident</th>\n",
       "      <th>Station_Area</th>\n",
       "      <th>Received_DtTm</th>\n",
       "      <th>Box</th>\n",
       "      <th>Original_Priority</th>\n",
       "      <th>Call_Type_Group</th>\n",
       "      <th>Number_of_Alarms</th>\n",
       "      <th>Unit_Type</th>\n",
       "      <th>Fire_Prevention_District</th>\n",
       "      <th>Supervisor_District</th>\n",
       "      <th>Received_month</th>\n",
       "      <th>Received_Hour</th>\n",
       "      <th>response_time</th>\n",
       "      <th>avg_response_history</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>94112</td>\n",
       "      <td>15</td>\n",
       "      <td>2018-10-15 06:28:00</td>\n",
       "      <td>8466</td>\n",
       "      <td>3</td>\n",
       "      <td>Potentially Life-Threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>ENGINE</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.031250</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>94103</td>\n",
       "      <td>08</td>\n",
       "      <td>2018-09-23 08:27:00</td>\n",
       "      <td>0231</td>\n",
       "      <td>A</td>\n",
       "      <td>Non Life-threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>PRIVATE</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>94109</td>\n",
       "      <td>03</td>\n",
       "      <td>2019-02-27 01:53:00</td>\n",
       "      <td>3223</td>\n",
       "      <td>2</td>\n",
       "      <td>Potentially Life-Threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>MEDIC</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.433333</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Medical Incident</td>\n",
       "      <td>94124</td>\n",
       "      <td>25</td>\n",
       "      <td>2019-02-21 01:59:00</td>\n",
       "      <td>6556</td>\n",
       "      <td>2</td>\n",
       "      <td>Potentially Life-Threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>CHIEF</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.173913</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other</td>\n",
       "      <td>94134</td>\n",
       "      <td>42</td>\n",
       "      <td>2018-04-12 05:45:00</td>\n",
       "      <td>6355</td>\n",
       "      <td>B</td>\n",
       "      <td>Potentially Life-Threatening</td>\n",
       "      <td>1</td>\n",
       "      <td>RESCUE CAPTAIN</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.142857</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Call_Type  Zipcode_of_Incident Station_Area       Received_DtTm  \\\n",
       "0  Medical Incident                94112           15 2018-10-15 06:28:00   \n",
       "1  Medical Incident                94103           08 2018-09-23 08:27:00   \n",
       "2  Medical Incident                94109           03 2019-02-27 01:53:00   \n",
       "3  Medical Incident                94124           25 2019-02-21 01:59:00   \n",
       "4             Other                94134           42 2018-04-12 05:45:00   \n",
       "\n",
       "    Box Original_Priority               Call_Type_Group  Number_of_Alarms  \\\n",
       "0  8466                 3  Potentially Life-Threatening                 1   \n",
       "1  0231                 A          Non Life-threatening                 1   \n",
       "2  3223                 2  Potentially Life-Threatening                 1   \n",
       "3  6556                 2  Potentially Life-Threatening                 1   \n",
       "4  6355                 B  Potentially Life-Threatening                 1   \n",
       "\n",
       "        Unit_Type Fire_Prevention_District Supervisor_District  \\\n",
       "0          ENGINE                        9                  11   \n",
       "1         PRIVATE                        3                   6   \n",
       "2           MEDIC                        4                   2   \n",
       "3           CHIEF                       10                  10   \n",
       "4  RESCUE CAPTAIN                        9                   9   \n",
       "\n",
       "   Received_month  Received_Hour  response_time  avg_response_history  label  \n",
       "0              10              6            6.0              7.031250    6.0  \n",
       "1               9              8            8.0              8.800000    8.0  \n",
       "2               2              1           10.0              9.433333   10.0  \n",
       "3               2              1           20.0              9.173913   20.0  \n",
       "4               4              5            8.0             10.142857    8.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1.select(['Call_Type', 'Zipcode_of_Incident', 'Station_Area', 'Received_DtTm',\n",
    "                  'Box', 'Original_Priority', 'Call_Type_Group', \n",
    "                  'Number_of_Alarms', 'Unit_Type', 'Fire_Prevention_District', \n",
    "                  'Supervisor_District', 'Received_month', 'Received_Hour', \n",
    "                  'response_time', 'avg_response_history', 'label'])\n",
    "\n",
    "df1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call_Type  =  0\n",
      "Zipcode_of_Incident  =  0\n",
      "Station_Area  =  0\n",
      "Received_DtTm  =  0\n",
      "Box  =  0\n",
      "Original_Priority  =  1\n",
      "Call_Type_Group  =  0\n",
      "Number_of_Alarms  =  0\n",
      "Unit_Type  =  0\n",
      "Fire_Prevention_District  =  0\n",
      "Supervisor_District  =  0\n",
      "Received_month  =  0\n",
      "Received_Hour  =  0\n",
      "response_time  =  0\n",
      "avg_response_history  =  1219\n",
      "label  =  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "298562"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in df1.columns:\n",
    "    print i,\" = \" , df1.filter(df1[i].isNull()).count()\n",
    "    \n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.filter(df1.avg_response_history.isNotNull()).filter(df1.Original_Priority.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train samples = ', 242879)\n",
      "('Test samples = ', 54463)\n"
     ]
    }
   ],
   "source": [
    "train_df = df1.filter(df1['Received_DtTm'] < '2019-01-01 00:00:00' ).drop('Received_DtTm')\n",
    "print('Train samples = ', train_df.count() )\n",
    "\n",
    "test_df = df1.filter(df1['Received_DtTm'] >= '2019-01-01 00:00:00' ).drop('Received_DtTm')\n",
    "print('Test samples = ', test_df.count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_vec_in = ['Call_Type','Zipcode_of_Incident','Call_Type_Group',\\\n",
    "                 'Unit_Type','Fire_Prevention_District',\\\n",
    "                 'Supervisor_District','Received_month','Received_Hour']\n",
    "\n",
    "column_vec_out = ['Call_Type_Vec','Zipcode_of_Incident_Vec','Call_Type_Group_Vec',\\\n",
    "                 'Unit_Type_Vec','Fire_Prevention_District_Vec',\\\n",
    "                 'Supervisor_District_Vec','Received_month_Vec','Received_Hour_Vec']\n",
    "\n",
    "indexers = [StringIndexer(inputCol=x, outputCol=x+'_indexed') \\\n",
    "           for x in column_vec_in]\n",
    "\n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol=x+'_indexed', outputCol=y) \\\n",
    "           for x,y in zip(column_vec_in, column_vec_out)]\n",
    "\n",
    "tmp = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp = [i for sublist in tmp for i in sublist]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cols_now = ['Call_Type_Vec','Zipcode_of_Incident_Vec','Call_Type_Group_Vec',\\\n",
    "            'Unit_Type_Vec','Fire_Prevention_District_Vec',\\\n",
    "            'Supervisor_District_Vec','Received_month_Vec','Received_Hour_Vec',\\\n",
    "            'avg_response_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_now = ['Call_Type_Vec','Zipcode_of_Incident_Vec','Call_Type_Group_Vec',\\\n",
    "            'Unit_Type_Vec','Fire_Prevention_District_Vec',\\\n",
    "            'Supervisor_District_Vec','Received_month_Vec','Received_Hour_Vec',\\\n",
    "            'avg_response_history']\n",
    "\n",
    "assembler_features = VectorAssembler(inputCols=cols_now, outputCol='features')\n",
    "\n",
    "tmp += [assembler_features]\n",
    "pipeline = Pipeline(stages=tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_train_df = pipeline.fit(train_df).transform(train_df).select(['label','features'])\n",
    "transformed_train_df.cache()\n",
    "\n",
    "transformed_test_df = pipeline.fit(test_df).transform(test_df).select(['label','features'])\n",
    "transformed_test_df.cache()\n",
    "transformed_test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242879, 54463)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train_df.count(), transformed_test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(numTrees=20, maxDepth=5, seed=2)\n",
    "model = rf.fit(transformed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",numTrees=20, maxDepth=5, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexer.\n",
    "model = rf.fit(transformed_train_df)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(transformed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['label', 'features', 'prediction'], 54463)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2631.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 500.0 failed 1 times, most recent failure: Lost task 0.0 in stage 500.0 (TID 5283, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IndexOutOfBoundsException: 108 not in [0,102)\n\tat breeze.linalg.SparseVector$mcD$sp.apply$mcD$sp(SparseVector.scala:74)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:73)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:49)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.SparseVector.apply$mcID$sp(SparseVector.scala:49)\n\tat org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:170)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:171)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:199)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:198)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3255)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3255)\n\tat sun.reflect.GeneratedMethodAccessor141.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IndexOutOfBoundsException: 108 not in [0,102)\n\tat breeze.linalg.SparseVector$mcD$sp.apply$mcD$sp(SparseVector.scala:74)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:73)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:49)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.SparseVector.apply$mcID$sp(SparseVector.scala:49)\n\tat org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:170)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:171)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:199)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:198)\n\t... 21 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-416767774d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \"\"\"\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \"\"\"\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2631.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 500.0 failed 1 times, most recent failure: Lost task 0.0 in stage 500.0 (TID 5283, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IndexOutOfBoundsException: 108 not in [0,102)\n\tat breeze.linalg.SparseVector$mcD$sp.apply$mcD$sp(SparseVector.scala:74)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:73)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:49)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.SparseVector.apply$mcID$sp(SparseVector.scala:49)\n\tat org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:170)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:171)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:199)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:198)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3255)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3255)\n\tat sun.reflect.GeneratedMethodAccessor141.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IndexOutOfBoundsException: 108 not in [0,102)\n\tat breeze.linalg.SparseVector$mcD$sp.apply$mcD$sp(SparseVector.scala:74)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:73)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:49)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.SparseVector.apply$mcID$sp(SparseVector.scala:49)\n\tat org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)\n\tat org.apache.spark.ml.tree.CategoricalSplit.shouldGoLeft(Split.scala:99)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:170)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:171)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:199)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:198)\n\t... 21 more\n"
     ]
    }
   ],
   "source": [
    "print(predictions.columns, predictions.count())\n",
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2455.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 470.0 failed 1 times, most recent failure: Lost task 7.0 in stage 470.0 (TID 5174, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IndexOutOfBoundsException: 114 not in [0,102)\n\tat breeze.linalg.SparseVector$mcD$sp.apply$mcD$sp(SparseVector.scala:74)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:73)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:49)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.SparseVector.apply$mcID$sp(SparseVector.scala:49)\n\tat org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)\n\tat org.apache.spark.ml.tree.ContinuousSplit.shouldGoLeft(Split.scala:161)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:170)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:199)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:198)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:57)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:54)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:109)\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:86)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IndexOutOfBoundsException: 114 not in [0,102)\n\tat breeze.linalg.SparseVector$mcD$sp.apply$mcD$sp(SparseVector.scala:74)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:73)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:49)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.SparseVector.apply$mcID$sp(SparseVector.scala:49)\n\tat org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)\n\tat org.apache.spark.ml.tree.ContinuousSplit.shouldGoLeft(Split.scala:161)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:170)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:199)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:198)\n\t... 34 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-57528cdd5e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m evaluator = RegressionEvaluator(\n\u001b[1;32m      3\u001b[0m     labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Root Mean Squared Error (RMSE) on test data = %g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/pyspark/ml/evaluation.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/pyspark/ml/evaluation.pyc\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shashank/anaconda2/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2455.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 470.0 failed 1 times, most recent failure: Lost task 7.0 in stage 470.0 (TID 5174, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IndexOutOfBoundsException: 114 not in [0,102)\n\tat breeze.linalg.SparseVector$mcD$sp.apply$mcD$sp(SparseVector.scala:74)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:73)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:49)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.SparseVector.apply$mcID$sp(SparseVector.scala:49)\n\tat org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)\n\tat org.apache.spark.ml.tree.ContinuousSplit.shouldGoLeft(Split.scala:161)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:170)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:199)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:198)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary$lzycompute(RegressionMetrics.scala:57)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.summary(RegressionMetrics.scala:54)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:109)\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:86)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1145)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1146)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IndexOutOfBoundsException: 114 not in [0,102)\n\tat breeze.linalg.SparseVector$mcD$sp.apply$mcD$sp(SparseVector.scala:74)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:73)\n\tat breeze.linalg.SparseVector$mcD$sp.apply(SparseVector.scala:49)\n\tat breeze.linalg.TensorLike$class.apply$mcID$sp(Tensor.scala:107)\n\tat breeze.linalg.SparseVector.apply$mcID$sp(SparseVector.scala:49)\n\tat org.apache.spark.ml.linalg.Vector$class.apply(Vectors.scala:102)\n\tat org.apache.spark.ml.linalg.SparseVector.apply(Vectors.scala:561)\n\tat org.apache.spark.ml.tree.ContinuousSplit.shouldGoLeft(Split.scala:161)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:170)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.tree.InternalNode.predictImpl(Node.scala:173)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$predict$1.apply(RandomForestRegressor.scala:208)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel.predict(RandomForestRegressor.scala:208)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:199)\n\tat org.apache.spark.ml.regression.RandomForestRegressionModel$$anonfun$3.apply(RandomForestRegressor.scala:198)\n\t... 34 more\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
